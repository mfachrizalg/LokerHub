{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re # Untuk regular expressions (membersihkan teks)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer # Atau WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB # Contoh model\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Mungkin perlu mengunduh data NLTK jika belum ada\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt') # Diperlukan untuk tokenisasi"
      ],
      "metadata": {
        "id": "8oiyTwbsRo0x"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive # Import library untuk mount Google Drive\n",
        "\n",
        "# --- LANGKAH PENTING: Mount Google Drive (jika belum) ---\n",
        "# Pastikan Google Drive sudah terhubung sebelum menjalankan ini\n",
        "try:\n",
        "    # Cek jika sudah di-mount untuk menghindari error jika dijalankan ulang\n",
        "    if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
        "      drive.mount('/content/drive')\n",
        "      print(\"Google Drive berhasil di-mount.\")\n",
        "    else:\n",
        "      print(\"Google Drive sudah di-mount sebelumnya.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal me-mount Google Drive: {e}\")\n",
        "    # Pertimbangkan untuk menghentikan eksekusi jika Drive gagal di-mount\n",
        "    # raise SystemExit(\"Mounting Google Drive gagal, tidak dapat melanjutkan.\")\n",
        "\n",
        "# --- Path File Zip dan Folder Tujuan Ekstraksi ---\n",
        "# Path file zip Anda di Google Drive\n",
        "zip_file_path = '/content/drive/MyDrive/senpro-ai/resume-dataset.zip'\n",
        "# Path folder tujuan ekstraksi di Google Drive (folder yang sama)\n",
        "extract_path = '/content/drive/MyDrive/senpro-ai/'\n",
        "\n",
        "# --- Proses Ekstraksi Langsung ke Google Drive dan Memuat Data ---\n",
        "try:\n",
        "    # Pastikan folder tujuan (extract_path) ada. Jika tidak, script zipfile akan membuatnya.\n",
        "    # Cek apakah file zip ada sebelum mencoba membuka\n",
        "    if not os.path.exists(zip_file_path):\n",
        "         print(f\"Error: File zip tidak ditemukan di path: {zip_file_path}\")\n",
        "         print(\"Pastikan path sudah benar dan Google Drive sudah di-mount.\")\n",
        "    else:\n",
        "        print(f\"Mencoba membuka file zip dari: {zip_file_path}\")\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            # Ekstrak semua isi zip ke folder extract_path di Google Drive\n",
        "            zip_ref.extractall(extract_path)\n",
        "        print(f\"File berhasil diekstrak ke: {extract_path}\")\n",
        "\n",
        "        # --- Mencari dan Memuat File CSV dari Lokasi Ekstraksi ---\n",
        "        csv_file = None\n",
        "        print(f\"Mencari file CSV di dalam: {extract_path}\")\n",
        "\n",
        "        # List file/folder yang ada di lokasi ekstraksi (langsung di Google Drive)\n",
        "        # Perlu waktu beberapa saat agar Drive sinkron setelah ekstraksi\n",
        "        import time\n",
        "        time.sleep(5) # Beri jeda sedikit agar Drive sync (opsional, bisa disesuaikan)\n",
        "\n",
        "        extracted_items = os.listdir(extract_path)\n",
        "        print(f\"Item di {extract_path}: {extracted_items}\")\n",
        "\n",
        "        for item in extracted_items:\n",
        "             full_item_path = os.path.join(extract_path, item)\n",
        "             # Cari file CSV langsung di folder senpro-ai\n",
        "             if os.path.isfile(full_item_path) and item.endswith(\".csv\"):\n",
        "                 csv_file = full_item_path\n",
        "                 print(f\"File CSV ditemukan: {csv_file}\")\n",
        "                 break\n",
        "             # Jika CSV ada di dalam subfolder hasil ekstraksi\n",
        "             elif os.path.isdir(full_item_path) and item != 'resume-dataset.zip': # Jangan masuk ke zip lagi\n",
        "                print(f\"Memeriksa subfolder: {full_item_path}\")\n",
        "                for sub_item in os.listdir(full_item_path):\n",
        "                     if sub_item.endswith(\".csv\"):\n",
        "                         csv_file = os.path.join(full_item_path, sub_item)\n",
        "                         print(f\"File CSV ditemukan di subfolder: {csv_file}\")\n",
        "                         break\n",
        "                if csv_file: break # Keluar loop luar jika sudah ketemu\n",
        "\n",
        "\n",
        "        if csv_file:\n",
        "            print(f\"Mencoba memuat file CSV dari: {csv_file}\")\n",
        "            df = pd.read_csv(csv_file)\n",
        "            print(\"\\nDataset berhasil dimuat:\")\n",
        "            print(df.head())\n",
        "            print(\"\\nInformasi Dataset:\")\n",
        "            df.info()\n",
        "            # Anda bisa melanjutkan ke langkah berikutnya dengan DataFrame df ini\n",
        "        else:\n",
        "            print(f\"Error: File CSV tidak ditemukan di {extract_path} atau subfoldernya setelah ekstraksi.\")\n",
        "            print(\"Pastikan file zip berisi file CSV dan cek isi folder `senpro-ai` di Google Drive Anda.\")\n",
        "\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: File di {zip_file_path} bukan file zip yang valid atau rusak.\")\n",
        "    print(\"Coba unduh ulang atau unggah ulang file tersebut ke Google Drive.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error kritis: Path tidak ditemukan. Pastikan {extract_path} atau {zip_file_path} valid.\")\n",
        "except Exception as e:\n",
        "     print(f\"Terjadi error lain saat ekstraksi atau load data: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lQEtZ5KTRf1",
        "outputId": "494033ee-83eb-4c51-e306-9fca8ffff8b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive sudah di-mount sebelumnya.\n",
            "Mencoba membuka file zip dari: /content/drive/MyDrive/senpro-ai/resume-dataset.zip\n",
            "File berhasil diekstrak ke: /content/drive/MyDrive/senpro-ai/\n",
            "Mencari file CSV di dalam: /content/drive/MyDrive/senpro-ai/\n",
            "Item di /content/drive/MyDrive/senpro-ai/: ['resume-dataset.zip', 'Resume', 'data']\n",
            "Memeriksa subfolder: /content/drive/MyDrive/senpro-ai/Resume\n",
            "File CSV ditemukan di subfolder: /content/drive/MyDrive/senpro-ai/Resume/Resume.csv\n",
            "Mencoba memuat file CSV dari: /content/drive/MyDrive/senpro-ai/Resume/Resume.csv\n",
            "\n",
            "Dataset berhasil dimuat:\n",
            "         ID                                         Resume_str  \\\n",
            "0  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
            "1  22323967           HR SPECIALIST, US HR OPERATIONS      ...   \n",
            "2  33176873           HR DIRECTOR       Summary      Over 2...   \n",
            "3  27018550           HR SPECIALIST       Summary    Dedica...   \n",
            "4  17812897           HR MANAGER         Skill Highlights  ...   \n",
            "\n",
            "                                         Resume_html Category  \n",
            "0  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
            "1  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
            "2  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
            "3  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
            "4  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
            "\n",
            "Informasi Dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2484 entries, 0 to 2483\n",
            "Data columns (total 4 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   ID           2484 non-null   int64 \n",
            " 1   Resume_str   2484 non-null   object\n",
            " 2   Resume_html  2484 non-null   object\n",
            " 3   Category     2484 non-null   object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 77.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "\n",
        "# --- TAMBAHKAN BARIS INI ---\n",
        "# Mengunduh paket stopwords, punkt, and punkt_tab (untuk tokenizer) dari NLTK\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    print(\"Resource 'stopwords' sudah ada.\")\n",
        "except LookupError:\n",
        "    print(\"Resource 'stopwords' belum ada, mengunduh...\")\n",
        "    nltk.download('stopwords', quiet=True) # quiet=True agar tidak terlalu verbose\n",
        "    print(\"Selesai mengunduh 'stopwords'.\")\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    print(\"Resource 'punkt' sudah ada.\")\n",
        "except LookupError:\n",
        "    print(\"Resource 'punkt' belum ada, mengunduh...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    print(\"Selesai mengunduh 'punkt'.\")\n",
        "\n",
        "# Download punkt_tab dataset\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "    print(\"Resource 'punkt_tab' sudah ada.\")\n",
        "except LookupError:\n",
        "    print(\"Resource 'punkt_tab' belum ada, mengunduh...\")\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    print(\"Selesai mengunduh 'punkt_tab'.\")\n",
        "\n",
        "# --------------------------\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# --- Sisa kode Langkah 3 Anda (seperti sebelumnya) ---\n",
        "\n",
        "# Gunakan DataFrame 'df' yang sudah dimuat dari langkah sebelumnya\n",
        "if 'df' in locals():\n",
        "    # Ambil kolom yang relevan\n",
        "    df['Resume_str'] = df['Resume_str'].fillna('')\n",
        "    df['Category'] = df['Category'].fillna('')\n",
        "\n",
        "    # Definisikan ulang fungsi pembersihan jika perlu\n",
        "    stop_words = set(stopwords.words('english')) # Baris ini sekarang seharusnya bekerja\n",
        "    def clean_resume_text(text):\n",
        "        text = re.sub(r'http\\S+', ' ', text)\n",
        "        text = re.sub(r'@\\S+', ' ', text)\n",
        "        text = re.sub(r'#\\S+', ' ', text)\n",
        "        text = re.sub(r'RT|cc', ' ', text)\n",
        "        text = re.sub(r'[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\"\"), ' ', text)\n",
        "        text = re.sub(r'[^\\x00-\\x7f]',r' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.lower()\n",
        "        words = word_tokenize(text) # Membutuhkan 'punkt'\n",
        "        cleaned_words = [word for word in words if word.isalpha() and word not in stop_words and len(word) > 1]\n",
        "        return ' '.join(cleaned_words)\n",
        "\n",
        "    # Terapkan pembersihan ke kolom 'Resume_str'\n",
        "    print(\"Membersihkan teks resume...\")\n",
        "    df['Cleaned_Resume'] = df['Resume_str'].apply(clean_resume_text)\n",
        "\n",
        "    print(\"\\nContoh Resume setelah dibersihkan:\")\n",
        "    print(df[['Resume_str', 'Cleaned_Resume']].head())\n",
        "\n",
        "    print(\"\\nJumlah data per Kategori:\")\n",
        "    print(df['Category'].value_counts())\n",
        "\n",
        "else:\n",
        "    print(\"Error: DataFrame 'df' tidak ditemukan. Jalankan langkah pemuatan data terlebih dahulu.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee6ba6c-593f-4c7b-96e4-476bc24cc9aa",
        "id": "sZ1ntKDIU5LC"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Resource 'stopwords' sudah ada.\n",
            "Resource 'punkt' sudah ada.\n",
            "Resource 'punkt_tab' belum ada, mengunduh...\n",
            "Selesai mengunduh 'punkt_tab'.\n",
            "Membersihkan teks resume...\n",
            "\n",
            "Contoh Resume setelah dibersihkan:\n",
            "                                          Resume_str  \\\n",
            "0           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
            "1           HR SPECIALIST, US HR OPERATIONS      ...   \n",
            "2           HR DIRECTOR       Summary      Over 2...   \n",
            "3           HR SPECIALIST       Summary    Dedica...   \n",
            "4           HR MANAGER         Skill Highlights  ...   \n",
            "\n",
            "                                      Cleaned_Resume  \n",
            "0  hr administrator marketing associate hr admini...  \n",
            "1  hr specialist us hr operations summary versati...  \n",
            "2  hr director summary years experience recruitin...  \n",
            "3  hr specialist summary dedicated driven dynamic...  \n",
            "4  hr manager skill highlights hr skills hr depar...  \n",
            "\n",
            "Jumlah data per Kategori:\n",
            "Category\n",
            "INFORMATION-TECHNOLOGY    120\n",
            "BUSINESS-DEVELOPMENT      120\n",
            "ADVOCATE                  118\n",
            "CHEF                      118\n",
            "ENGINEERING               118\n",
            "ACCOUNTANT                118\n",
            "FINANCE                   118\n",
            "FITNESS                   117\n",
            "AVIATION                  117\n",
            "SALES                     116\n",
            "BANKING                   115\n",
            "HEALTHCARE                115\n",
            "CONSULTANT                115\n",
            "CONSTRUCTION              112\n",
            "PUBLIC-RELATIONS          111\n",
            "HR                        110\n",
            "DESIGNER                  107\n",
            "ARTS                      103\n",
            "TEACHER                   102\n",
            "APPAREL                    97\n",
            "DIGITAL-MEDIA              96\n",
            "AGRICULTURE                63\n",
            "AUTOMOBILE                 36\n",
            "BPO                        22\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "if 'df' in locals() and 'Cleaned_Resume' in df.columns:\n",
        "    # Inisialisasi TF-IDF Vectorizer\n",
        "    # Anda bisa mengatur parameter seperti max_features, ngram_range, dll.\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, # Batasi jumlah fitur (kata unik)\n",
        "                                       ngram_range=(1,2)) # Pertimbangkan 1 kata dan 2 kata berurutan\n",
        "\n",
        "    print(\"\\nMembuat matriks TF-IDF...\")\n",
        "    # Buat matriks TF-IDF dari teks resume yang bersih\n",
        "    X = tfidf_vectorizer.fit_transform(df['Cleaned_Resume'])\n",
        "\n",
        "    # Target variabel (kategori pekerjaan)\n",
        "    y = df['Category']\n",
        "\n",
        "    print(\"Matriks TF-IDF (fitur) berhasil dibuat.\")\n",
        "    print(\"Ukuran matriks X:\", X.shape)\n",
        "    print(\"Ukuran target y:\", y.shape)\n",
        "\n",
        "    # Simpan vocabulary (kata-kata unik) jika perlu dilihat\n",
        "    # feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    # print(\"Contoh fitur (kata):\", feature_names[:20])\n",
        "\n",
        "else:\n",
        "    print(\"Error: DataFrame 'df' atau kolom 'Cleaned_Resume' tidak ditemukan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1ENqAatU2rw",
        "outputId": "03a0b37b-ef1c-4a6a-d19a-a3f93f749bb5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Membuat matriks TF-IDF...\n",
            "Matriks TF-IDF (fitur) berhasil dibuat.\n",
            "Ukuran matriks X: (2484, 5000)\n",
            "Ukuran target y: (2484,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if 'X' in locals() and 'y' in locals():\n",
        "    # Bagi data (80% latih, 20% uji)\n",
        "    # stratify=y penting untuk menjaga proporsi kelas di kedua set jika data tidak seimbang\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                        test_size=0.2,\n",
        "                                                        random_state=42,\n",
        "                                                        stratify=y)\n",
        "\n",
        "    print(\"\\nData berhasil dibagi:\")\n",
        "    print(\"Ukuran data latih (X_train):\", X_train.shape)\n",
        "    print(\"Ukuran data uji (X_test):\", X_test.shape)\n",
        "    print(\"Ukuran label latih (y_train):\", y_train.shape)\n",
        "    print(\"Ukuran label uji (y_test):\", y_test.shape)\n",
        "else:\n",
        "    print(\"Error: Variabel X atau y belum terdefinisi. Jalankan langkah sebelumnya.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFKNJ6AAVbI5",
        "outputId": "257fa72d-f449-4f34-8c41-3d4c71a49764"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data berhasil dibagi:\n",
            "Ukuran data latih (X_train): (1987, 5000)\n",
            "Ukuran data uji (X_test): (497, 5000)\n",
            "Ukuran label latih (y_train): (1987,)\n",
            "Ukuran label uji (y_test): (497,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "if 'X_train' in locals():\n",
        "    # Pilih model\n",
        "    model = MultinomialNB()\n",
        "\n",
        "    print(\"\\nMelatih model Multinomial Naive Bayes...\")\n",
        "    # Latih model\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Model berhasil dilatih.\")\n",
        "\n",
        "    # Evaluasi cepat di data latih (opsional, untuk cek overfitting)\n",
        "    # y_train_pred = model.predict(X_train)\n",
        "    # print(f\"Akurasi di data latih: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: Data latih (X_train, y_train) belum terdefinisi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WStPnPRgVeju",
        "outputId": "dae49c10-5e50-401f-a8df-317c38413add"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Melatih model Multinomial Naive Bayes...\n",
            "Model berhasil dilatih.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'model' in locals() and 'X_test' in locals():\n",
        "    print(\"\\nMengevaluasi model pada data uji...\")\n",
        "    # Prediksi pada data uji\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluasi performa\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\nAkurasi Model pada Data Uji: {accuracy:.4f}\")\n",
        "\n",
        "    print(\"\\nLaporan Klasifikasi Lengkap:\")\n",
        "    # target_names bisa diisi dengan list unik dari df['Category'].unique() jika mau nama kelas di laporan\n",
        "    print(classification_report(y_test, y_pred)) #, target_names=df['Category'].unique()))\n",
        "\n",
        "    # Bisa juga tampilkan Confusion Matrix\n",
        "    # from sklearn.metrics import confusion_matrix\n",
        "    # import seaborn as sns\n",
        "    # import matplotlib.pyplot as plt\n",
        "    # cm = confusion_matrix(y_test, y_pred)\n",
        "    # plt.figure(figsize=(12, 10))\n",
        "    # sns.heatmap(cm, annot=True, fmt='d', xticklabels=model.classes_, yticklabels=model.classes_)\n",
        "    # plt.xlabel('Predicted')\n",
        "    # plt.ylabel('Actual')\n",
        "    # plt.show()\n",
        "\n",
        "else:\n",
        "     print(\"Error: Model atau data uji belum siap untuk evaluasi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NUrNkyOV0p8",
        "outputId": "4f41ed24-76a5-463d-e366-88f9a4161bc5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mengevaluasi model pada data uji...\n",
            "\n",
            "Akurasi Model pada Data Uji: 0.5714\n",
            "\n",
            "Laporan Klasifikasi Lengkap:\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "            ACCOUNTANT       0.53      0.88      0.66        24\n",
            "              ADVOCATE       0.33      0.42      0.37        24\n",
            "           AGRICULTURE       1.00      0.08      0.14        13\n",
            "               APPAREL       0.50      0.05      0.10        19\n",
            "                  ARTS       0.75      0.14      0.24        21\n",
            "            AUTOMOBILE       0.00      0.00      0.00         7\n",
            "              AVIATION       0.83      0.62      0.71        24\n",
            "               BANKING       0.87      0.57      0.68        23\n",
            "                   BPO       0.00      0.00      0.00         4\n",
            "  BUSINESS-DEVELOPMENT       0.41      0.88      0.56        24\n",
            "                  CHEF       0.81      0.71      0.76        24\n",
            "          CONSTRUCTION       0.68      0.77      0.72        22\n",
            "            CONSULTANT       1.00      0.09      0.16        23\n",
            "              DESIGNER       0.76      0.62      0.68        21\n",
            "         DIGITAL-MEDIA       0.75      0.32      0.44        19\n",
            "           ENGINEERING       0.68      0.62      0.65        24\n",
            "               FINANCE       0.67      0.42      0.51        24\n",
            "               FITNESS       0.75      0.65      0.70        23\n",
            "            HEALTHCARE       0.52      0.61      0.56        23\n",
            "                    HR       0.67      0.91      0.77        22\n",
            "INFORMATION-TECHNOLOGY       0.57      1.00      0.73        24\n",
            "      PUBLIC-RELATIONS       0.45      0.77      0.57        22\n",
            "                 SALES       0.35      0.57      0.43        23\n",
            "               TEACHER       0.50      0.80      0.62        20\n",
            "\n",
            "              accuracy                           0.57       497\n",
            "             macro avg       0.60      0.52      0.49       497\n",
            "          weighted avg       0.63      0.57      0.54       497\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Pastikan Google Drive masih terhubung\n",
        "# Tentukan path untuk menyimpan model dan vectorizer di folder senpro-ai\n",
        "save_dir = '/content/drive/MyDrive/senpro-ai/'\n",
        "model_path = os.path.join(save_dir, 'resume_category_model.joblib')\n",
        "vectorizer_path = os.path.join(save_dir, 'tfidf_vectorizer.joblib')\n",
        "\n",
        "if 'model' in locals() and 'tfidf_vectorizer' in locals():\n",
        "    try:\n",
        "        print(f\"\\nMenyimpan model ke: {model_path}\")\n",
        "        joblib.dump(model, model_path)\n",
        "        print(\"Model berhasil disimpan.\")\n",
        "\n",
        "        print(f\"Menyimpan TF-IDF vectorizer ke: {vectorizer_path}\")\n",
        "        joblib.dump(tfidf_vectorizer, vectorizer_path)\n",
        "        print(\"Vectorizer berhasil disimpan.\")\n",
        "\n",
        "        print(\"\\nKedua file (model dan vectorizer) telah disimpan di Google Drive.\")\n",
        "        print(\"Anda akan memerlukan kedua file ini untuk aplikasi backend/frontend.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Terjadi error saat menyimpan file: {e}\")\n",
        "else:\n",
        "    print(\"Error: Model atau TF-IDF Vectorizer belum dilatih/dibuat.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5V4dosDV6XH",
        "outputId": "3af0007e-4e05-4b79-97e1-63753a3a45ee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Menyimpan model ke: /content/drive/MyDrive/senpro-ai/resume_category_model.joblib\n",
            "Model berhasil disimpan.\n",
            "Menyimpan TF-IDF vectorizer ke: /content/drive/MyDrive/senpro-ai/tfidf_vectorizer.joblib\n",
            "Vectorizer berhasil disimpan.\n",
            "\n",
            "Kedua file (model dan vectorizer) telah disimpan di Google Drive.\n",
            "Anda akan memerlukan kedua file ini untuk aplikasi backend/frontend.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install PyPDF2\n",
        "!pip install nltk\n",
        "import nltk\n",
        "import PyPDF2\n",
        "import joblib\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "\n",
        "# Download stopwords if not already present\n",
        "nltk.download('stopwords', quiet=True)\n",
        "# Download the 'punkt_tab' dataset\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# Load model dan vectorizer (sesuaikan path jika perlu)\n",
        "model_path = '/content/drive/MyDrive/senpro-ai/resume_category_model.joblib'\n",
        "vectorizer_path = '/content/drive/MyDrive/senpro-ai/tfidf_vectorizer.joblib'\n",
        "model = joblib.load(model_path)\n",
        "tfidf_vectorizer = joblib.load(vectorizer_path)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_resume_text(text):\n",
        "    text = re.sub(r'http\\S+', ' ', text)\n",
        "    text = re.sub(r'@\\S+', ' ', text)\n",
        "    text = re.sub(r'#\\S+', ' ', text)\n",
        "    text = re.sub(r'RT|cc', ' ', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7f]',r' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    words = word_tokenize(text)\n",
        "    cleaned_words = [word for word in words if word.isalpha() and word not in stop_words and len(word) > 1]\n",
        "    return ' '.join(cleaned_words)\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(reader.pages)):\n",
        "                text += reader.pages[page_num].extract_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "    return text\n",
        "\n",
        "def predict_category(resume_text):\n",
        "    cleaned_text = clean_resume_text(resume_text)\n",
        "    text_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
        "    predicted_category = model.predict(text_vectorized)[0]\n",
        "    return predicted_category\n",
        "\n",
        "# Contoh: Asumsikan PDF ada di subfolder \"Resume\" di Google Drive Anda\n",
        "# Ambil daftar file di direktori\n",
        "pdf_directory = '/content/drive/MyDrive/senpro-ai/'  # Path to the directory containing the PDF\n",
        "pdf_file_name = 'Resume.pdf'  # Name of the PDF file\n",
        "\n",
        "# Define pdf_files here by listing the PDF files in the directory\n",
        "pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]\n",
        "\n",
        "\n",
        "if pdf_files:\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_file_path = os.path.join(pdf_directory, pdf_file)\n",
        "        if os.path.exists(pdf_file_path):\n",
        "            resume_text = extract_text_from_pdf(pdf_file_path)\n",
        "            if resume_text:\n",
        "                predicted_category = predict_category(resume_text)\n",
        "                print(f\"\\nFile: {pdf_file}\")\n",
        "                print(f\"Kategori yang Diprediksi: {predicted_category}\")\n",
        "            else:\n",
        "                print(f\"Gagal mengekstrak teks dari {pdf_file}\")\n",
        "        else:\n",
        "            print(f\"Error: File tidak ditemukan di {pdf_file_path}\")\n",
        "else:\n",
        "    print(\"Tidak ada file PDF ditemukan di direktori yang ditentukan.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7yFphTKafc8",
        "outputId": "f7270b8d-7486-4c9a-f2bc-0c157b24bd49"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "\n",
            "File: Resume.pdf\n",
            "Kategori yang Diprediksi: SALES\n",
            "\n",
            "File: CV ATS Muhammad Rendy.pdf\n",
            "Kategori yang Diprediksi: INFORMATION-TECHNOLOGY\n",
            "\n",
            "File: Blue Simple Modern Resume.pdf\n",
            "Kategori yang Diprediksi: INFORMATION-TECHNOLOGY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CgJ74stLZ_vt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}